{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import gc\n",
    "import xgboost as xgb\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_num_features = 10\n",
    "pad_size = 1\n",
    "boundary_letter = -1 \n",
    "#It's to differentiate the words in one sliding window :) Because all characters are greater than 0 so using -1 will \n",
    "#help decision trees to distinguish bondaries better.\n",
    "space_letter = 0\n",
    "round_num = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_name = \"C:/Users/CHICHI/Desktop/文本正则化/input/en_train.csv\"\n",
    "test_file_name = \"C:/Users/CHICHI/Desktop/文本正则化/input/en_test_2.csv\"\n",
    "model_file_name = \"C:/Users/CHICHI/Desktop/文本正则化/model_vars/train16.v2.6.model\"\n",
    "model_dump_name = \"C:/Users/CHICHI/Desktop/文本正则化/model_vars/dump.train16.v2.6.txt\"\n",
    "class_pred_file_name = \"C:/Users/CHICHI/Desktop/文本正则化/output/class_pred_16.v2.6.csv\"\n",
    "all_pred_file_name = \"C:/Users/CHICHI/Desktop/文本正则化/output/train_pred.v2.6.csv\"\n",
    "valid_compare_file_name = \"C:/Users/CHICHI/Desktop/文本正则化/output/valid_compare_16.v2.6.csv\"\n",
    "train_compare_file_name = \"C:/Users/CHICHI/Desktop/文本正则化/output/train_compare_16.v2.6.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'objective': 'multi:softprob',\n",
    "         'eta': '0.2',\n",
    "         'max_depth': 11,\n",
    "         'silent': True,\n",
    "         'num_class': 16,\n",
    "         'eval_metric': 'merror'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['PLAIN', 'PUNCT', 'DATE', 'LETTERS', 'CARDINAL', 'VERBATIM',\n",
    "          'DECIMAL', 'MEASURE', 'MONEY', 'ORDINAL', 'TIME', 'ELECTRONIC',\n",
    "          'DIGIT', 'FRACTION', 'TELEPHONE', 'ADDRESS']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_dict = {'PLAIN': 0.01,\n",
    "               'PUNCT': 1,\n",
    "               'DATE': 1,\n",
    "               'LETTERS': 1,\n",
    "               'CARDINAL': 1,\n",
    "               'VERBATIM': 1,\n",
    "               'DECIMAL': 1,\n",
    "               'MEASURE': 1,\n",
    "               'MONEY': 1,\n",
    "               'ORDINAL': 1,\n",
    "               'TIME': 1,\n",
    "               'ELECTRONIC': 1,\n",
    "               'DIGIT': 1,\n",
    "               'FRACTION': 1,\n",
    "               'TELEPHONE': 1,\n",
    "               'ADDRESS': 1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n(1)\\nIt groups previous word and posterior word into a fixed size array. Because the decision tree will predict more accurately \\nwith the context of the word than without context, same as a recurrent neural network. And it has to be a fixed-size array \\nbecause XGboost doesn't accept variable input length.\\n(2)\\nIt use ASCII encoding instead of other encodings (eg. TF-IDF) because I think labeling the words doesn't require decision \\ntree to understand the meaning of each words, that means a simple ASCII encoding will do the trick.\\n(3)\\nAnd tt requires every word to include its previous and posterior words (if a word doesn't have a previous word or posterior \\nword, then use 0 to represent those missing words).Because we need to tell decision tree that it needs to make predictions \\nbased mostly on the middle word. Say we have arrays like this: array A(a cat is), B(cat is 10), C(is 10 years) D(10 years old). \\nWe need XGBoost to tell use A is LETTER, B is LETTER and C is NUMBER. And XGBoost can see the difference between A and C in that \\nC's middle word is not the same type as A's.\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def context_window_transform(data, pad_size):\n",
    "    \"\"\"每个词加上前面一个和后面一个词，中间用-1隔开\"\"\"\n",
    "    pre = np.zeros(max_num_features, dtype=int)\n",
    "    pre = [pre for x in np.arange(pad_size)]\n",
    "    data = pre + data + pre\n",
    "    # print(data)\n",
    "    new_data = []\n",
    "    for i in np.arange(len(data) - pad_size * 2):\n",
    "        if np.all(data[i + pad_size] == 0):\n",
    "            continue\n",
    "        row = []\n",
    "        for x in data[i: i + pad_size * 2 + 1]:\n",
    "            row += [boundary_letter]\n",
    "            row += x.tolist()\n",
    "        row += [boundary_letter]\n",
    "        new_data.append(row)\n",
    "    return new_data\n",
    "\"\"\"\n",
    "(1)\n",
    "It groups previous word and posterior word into a fixed size array. Because the decision tree will predict more accurately \n",
    "with the context of the word than without context, same as a recurrent neural network. And it has to be a fixed-size array \n",
    "because XGboost doesn't accept variable input length.\n",
    "(2)\n",
    "It use ASCII encoding instead of other encodings (eg. TF-IDF) because I think labeling the words doesn't require decision \n",
    "tree to understand the meaning of each words, that means a simple ASCII encoding will do the trick.\n",
    "(3)\n",
    "And tt requires every word to include its previous and posterior words (if a word doesn't have a previous word or posterior \n",
    "word, then use 0 to represent those missing words).Because we need to tell decision tree that it needs to make predictions \n",
    "based mostly on the middle word. Say we have arrays like this: array A(a cat is), B(cat is 10), C(is 10 years) D(10 years old). \n",
    "We need XGBoost to tell use A is LETTER, B is LETTER and C is NUMBER. And XGBoost can see the difference between A and C in that \n",
    "C's middle word is not the same type as A's.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feas(data_df):\n",
    "    # 特征工程\n",
    "    feas = []\n",
    "    # 1.token的长度\n",
    "    fea_len = data_df[\"before\"].apply(lambda token: len(str(token))).values\n",
    "    feas.append(fea_len)\n",
    "    # 2.是否是每句话的第一个token\n",
    "    fea_start = data_df[\"token_id\"].apply(lambda token_id: 1 if int(token_id) == 0 else 0)\n",
    "    feas.append(fea_start)\n",
    "    fea_x = np.transpose(np.vstack(feas))\n",
    "    return fea_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(with_valid=True, save=True):\n",
    "    print(\"open data files ...\")\n",
    "    train_df = pd.read_csv(train_file_name)\n",
    "\n",
    "    print(\"data processing...\")\n",
    "    x_data = []\n",
    "    # 将类别数字化\n",
    "    # labels = train_df[\"class\"].unique()\n",
    "    class2index = dict(zip(labels, range(len(labels))))\n",
    "    y_data = list(map(lambda c: class2index[c], train_df['class'].values))# 'map' object is not subscriptable, 需要转换成list\n",
    "    gc.collect()\n",
    "    # 每个目标词用组成这个词的所有字符的ascii码表示，并padding\n",
    "    for x, token_id in zip(train_df['before'].values, train_df[\"token_id\"].values):\n",
    "        if token_id == 0:\n",
    "            x_row_before = np.zeros(max_num_features, dtype=int)\n",
    "            x_data.append(x_row_before)\n",
    "        x_row = np.ones(max_num_features, dtype=int) * space_letter\n",
    "\n",
    "        for xi, i in zip(list(str(x)), np.arange(max_num_features)):\n",
    "            x_row[i] = ord(xi)\n",
    "        x_data.append(x_row)\n",
    "\n",
    "    fea_x = get_feas(train_df)\n",
    "\n",
    "    del train_df\n",
    "    gc.collect()\n",
    "\n",
    "    x_data_context = np.array(context_window_transform(x_data, pad_size))\n",
    "    del x_data\n",
    "    gc.collect()\n",
    "    # x_data_context_a = np.array(x_data_context)\n",
    "    x_data_context_a = np.hstack([x_data_context, fea_x])\n",
    "    y_data_a = np.array(list(y_data))\n",
    "\n",
    "    # 计算每个类别的权重\n",
    "    print(np.unique(y_data_a))\n",
    "    index_weight_dict = dict([(class2index[k], v)for k, v in weight_dict.items()])\n",
    "    class_weights = class_weight.compute_class_weight(\"balanced\", np.arange(16), y_data_a)# array不接收迭代器，需要转换成list\n",
    "    weights = np.array(map(lambda y: class_weights[y], y_data_a))\n",
    "    # print(\"weights: \", weights[:100])\n",
    "    print('Total number of samples:', len(x_data_context))\n",
    "\n",
    "    print('x_data sample:')\n",
    "    print(x_data_context[0])\n",
    "    print('y_data sample:')\n",
    "    print(y_data[0])\n",
    "    print('labels:')\n",
    "    print(labels)\n",
    "\n",
    "    del x_data_context\n",
    "    del y_data\n",
    "    gc.collect()\n",
    "\n",
    "    if with_valid:\n",
    "        x_train, x_valid, y_train, y_valid = train_test_split(x_data_context_a, y_data_a,\n",
    "                                                              test_size=0.01, random_state=2017)\n",
    "        del x_data_context_a\n",
    "        del y_data_a\n",
    "        gc.collect()\n",
    "\n",
    "        print(\"forming dmatrix...\")\n",
    "        dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "        dvalid = xgb.DMatrix(x_valid, label=y_valid)\n",
    "        watchlist = [(dvalid, 'valid'), (dtrain, 'train')]\n",
    "\n",
    "        del x_train\n",
    "        del y_train\n",
    "        gc.collect()\n",
    "\n",
    "        print(\"training start...\")\n",
    "        print(\"params: \", param)\n",
    "        # print(\"loading model ...\")\n",
    "        model = xgb.train(param, dtrain, round_num, watchlist,\n",
    "                          # xgb_model=\"C:/Users/CHICHI/Desktop/文本正则化/output/train16.v2.5.model\",\n",
    "                          early_stopping_rounds=10,\n",
    "                          verbose_eval=10)\n",
    "    else:\n",
    "        dtrain = xgb.DMatrix(x_data_context_a, label=y_data_a)\n",
    "        watchlist = [(dtrain, 'train')]\n",
    "        del x_data_context_a\n",
    "        del y_data_a\n",
    "        gc.collect()\n",
    "        model = xgb.train(param, dtrain, round_num, watchlist, early_stopping_rounds=20,\n",
    "                          verbose_eval=10)\n",
    "    if save:\n",
    "        model.save_model(model_file_name)\n",
    "        model.dump_model(model_dump_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open data files ...\n",
      "data processing...\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15]\n",
      "Total number of samples: 9918441\n",
      "x_data sample:\n",
      "[ -1   0   0   0   0   0   0   0   0   0   0  -1  66 114 105 108 108  97\n",
      " 110 116  97 105  -1 105 115   0   0   0   0   0   0   0   0  -1]\n",
      "y_data sample:\n",
      "0\n",
      "labels:\n",
      "['PLAIN', 'PUNCT', 'DATE', 'LETTERS', 'CARDINAL', 'VERBATIM', 'DECIMAL', 'MEASURE', 'MONEY', 'ORDINAL', 'TIME', 'ELECTRONIC', 'DIGIT', 'FRACTION', 'TELEPHONE', 'ADDRESS']\n",
      "forming dmatrix...\n",
      "training start...\n",
      "params:  {'objective': 'multi:softprob', 'eta': '0.2', 'max_depth': 11, 'silent': True, 'num_class': 16, 'eval_metric': 'merror'}\n",
      "[09:47:33] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:47:36] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\gbm\\gbtree.cc:139: Tree method is automatically selected to be 'approx' for faster speed. To use old behavior (exact greedy algorithm on single machine), set tree_method to 'exact'.\n",
      "[0]\tvalid-merror:0.00888\ttrain-merror:0.00844\n",
      "Multiple eval metrics have been passed: 'train-merror' will be used for early stopping.\n",
      "\n",
      "Will train until train-merror hasn't improved in 10 rounds.\n",
      "[10]\tvalid-merror:0.00512\ttrain-merror:0.00458\n",
      "[20]\tvalid-merror:0.00401\ttrain-merror:0.00343\n",
      "[30]\tvalid-merror:0.00345\ttrain-merror:0.00271\n",
      "[40]\tvalid-merror:0.00308\ttrain-merror:0.00229\n",
      "[49]\tvalid-merror:0.00287\ttrain-merror:0.00198\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    test_df = pd.read_csv(test_file_name)\n",
    "    # 每个目标词用组成这个词的所有字符的ascii码表示，并padding\n",
    "    print(\"loading test data ...\")\n",
    "    x_data = []\n",
    "    for x, token_id in zip(test_df['before'].values, test_df[\"token_id\"].values):\n",
    "        if token_id == 0:\n",
    "            x_row_before = np.zeros(max_num_features, dtype=int)\n",
    "            x_data.append(x_row_before)\n",
    "        x_row = np.ones(max_num_features, dtype=int) * space_letter\n",
    "\n",
    "        for xi, i in zip(list(str(x)), np.arange(max_num_features)):\n",
    "            x_row[i] = ord(xi)\n",
    "        x_data.append(x_row)\n",
    "\n",
    "    fea_x = get_feas(test_df)\n",
    "    x_data_context = np.array(context_window_transform(x_data, pad_size))\n",
    "    # x_data_context_a = np.array(x_data_context)\n",
    "    x_data_context_a = np.hstack([x_data_context, fea_x])\n",
    "    dtest = xgb.DMatrix(x_data_context_a)\n",
    "    print(\"loading model ...\")\n",
    "    bst = xgb.Booster(param)  # init model\n",
    "    bst.load_model(model_file_name)\n",
    "    print(\"start predicting ...\")\n",
    "    # ypred = bst.predict(dtest)\n",
    "\n",
    "    yprob = bst.predict(dtest)\n",
    "    ypred = np.argmax(yprob, axis=1)\n",
    "    ymax_prob = np.max(yprob, axis=1)\n",
    "    print(\"ypred:\", np.shape(ypred))\n",
    "    print(\"ymax_prob:\", np.shape(ymax_prob))\n",
    "    # print(test_df.shape)\n",
    "    # print(test_df[\"sentence_id\"].values.shape, test_df[\"sentence_id\"].values.dtype)\n",
    "    source_word = test_df.before\n",
    "    sentence_id = test_df.sentence_id\n",
    "    token_id = test_df.token_id\n",
    "    ids_a = [str(\"\"+str(e[0])+\"_\"+str(e[1])) for e in list(zip(sentence_id,token_id))]\n",
    "    print(\"ids_a: \", ids_a)\n",
    "    test_df[\"id\"] = ids_a\n",
    "    class_df = test_df[[\"id\", \"before\"]]\n",
    "    class_df[\"class_pred\"] = ypred\n",
    "    class_df[\"max_prob\"] = ymax_prob\n",
    "    class_df.to_csv(class_pred_file_name, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading test data ...\n",
      "loading model ...\n",
      "start predicting ...\n",
      "[14:52:02] WARNING: C:\\Users\\Administrator\\workspace\\xgboost-win64_release_1.2.0\\src\\learner.cc:516: \n",
      "Parameters: { silent } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "ypred: (956046,)\n",
      "ymax_prob: (956046,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "<ipython-input-47-e53427d6e954>:41: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  class_df[\"class_pred\"] = ypred\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
